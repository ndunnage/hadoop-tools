# Licensed to the Apache Software Foundation (ASF) under one
# or more contributor license agreements.  See the NOTICE file
# distributed with this work for additional information
# regarding copyright ownership.  The ASF licenses this file
# to you under the Apache License, Version 2.0 (the
# "License"); you may not use this file except in compliance
# with the License.  You may obtain a copy of the License at
#
#  http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing,
# software distributed under the License is distributed on an
# "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
# KIND, either express or implied.  See the License for the
# specific language governing permissions and limitations
# under the License.

# Application configuration file in HOCON format (Human-Optimized Config Object Notation). 
# HOCON syntax is defined at http://github.com/typesafehub/config/blob/master/HOCON.md
# and also used by Akka (http://www.akka.io) and Play (http://www.playframework.org/).
# For more examples see http://doc.akka.io/docs/akka/2.1.2/general/configuration.html

# morphline.conf example file
# this is a comment

# Specify server locations in a SOLR_LOCATOR variable; used later in variable substitutions:
SOLR_LOCATOR : {
  # Name of solr collection
  collection : syslogs
  
  # ZooKeeper ensemble
  zkHost : "$ZK_HOST"
  
  # Relative or absolute path to a directory containing conf/solrconfig.xml and conf/schema.xml
  # If this path is uncommented it takes precedence over the configuration stored in ZooKeeper.  
  # solrHomeDir : "example/solr/collection1"
  
  # The maximum number of documents to send to Solr per network batch (throughput knob)
  # batchSize : 100
}

# Specify an array of one or more morphlines, each of which defines an ETL 
# transformation chain. A morphline consists of one or more (potentially 
# nested) commands. A morphline is a way to consume records (e.g. Flume events, 
# HDFS files or blocks), turn them into a stream of records, and pipe the stream 
# of records through a set of easily configurable transformations on it's way to 
# Solr (or a MapReduceIndexerTool RecordWriter that feeds via a Reducer into Solr).
morphlines : [
  {
    # Name used to identify a morphline. E.g. used if there are multiple morphlines in a 
    # morphline config file
    id : morphline1 
    
    # Import all morphline commands in these java packages and their subpackages.
    # Other commands that may be present on the classpath are not visible to this morphline.
    importCommands : ["org.kitesdk.**", "org.apache.solr.**"]
    
    commands : [                    
      { 
        
      # Extract the syslog files
      grok { 
          dictionaryFiles : [target/test-classes/grok-dictionaries]
          dictionaryString : """
            XUUID [A-Fa-f0-9]{8}-(?:[A-Fa-f0-9]{4}-){3}[A-Fa-f0-9]{12}
          """
                               
          expressions : { 
            message : """<%{POSINT:syslog_pri}>%{SYSLOGTIMESTAMP:syslog_timestamp} %{SYSLOGHOST:syslog_hostname} %{DATA:syslog_program}(?:\[%{POSINT:syslog_pid}\])?: %{GREEDYDATA:syslog_message}"""
            
            #message2 : "(?<queue_field>.*)" 
            #message3 : "(?<queue_field:int>.*)" 
            #message4 : "%{NUMBER:queue_field:int}" 
          }
          extract : true
          numRequiredMatches : ${numRequiredMatches}
          findSubstrings : false
          addEmptyStrings : false
        }


      # Consume the output record of the previous command and pipe another record downstream.
      #
      # convert timestamp field to native Solr timestamp format
      # e.g. 2012-09-06T07:14:34Z to 2012-09-06T07:14:34.000Z
      {
        convertTimestamp {
          field : created_at
          inputFormats : ["yyyy-MM-dd'T'HH:mm:ss'Z'", "yyyy-MM-dd"]
          inputTimezone : America/Los_Angeles
#          outputFormat : "yyyy-MM-dd'T'HH:mm:ss.SSSZ"                                 
          outputTimezone : UTC
        }
      }
      
      # Consume the output record of the previous command and pipe another record downstream.
      #
      # Command that sanitizes record fields that are unknown to Solr schema.xml by either 
      # deleting them (renameToPrefix is absent or a zero length string), or by moving them to a
      # field prefixed with the given renameToPrefix (e.g. renameToPrefix = "ignored_" to use 
      # typical dynamic Solr fields).
      #
      # Recall that Solr throws an exception on any attempt to load a document that contains a 
      # field that isn't specified in schema.xml.
      {
        sanitizeUnknownSolrFields {
          # Location from which to fetch Solr schema
          solrLocator : ${SOLR_LOCATOR}
          
          # renameToPrefix : "ignored_"
        }
      }  
            
      # log the record at DEBUG level to SLF4J
      { logDebug { format : "output record: {}", args : ["@{}"] } }    
      
      # load the record into a SolrServer or MapReduce SolrOutputFormat.
      { 
        loadSolr {
          solrLocator : ${SOLR_LOCATOR}
        }
      }
    ]
  }
]